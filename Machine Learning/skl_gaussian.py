import numpy as np
import matplotlib.pyplot as plt 
import matplotlib as mpl
import math
from sklearn import mixture 
from matplotlib.patches import Ellipse
import itertools

def generate_data(mean_list, covariance_list, num_list):
    np.random.seed(0)
    data = np.random.multivariate_normal(mean_list[0], covariance_list[0], num_list[0])
    for i in range(len(mean_list)-1):
        new_data = np.random.multivariate_normal(mean_list[i+1], covariance_list[i+1], num_list[i+1])
        data = np.append(data, new_data, 0)
    return data

def distribution_plot(data, num_list):
    color_list = ['hotpink', 'slateblue', 'aquamarine']
    x = data[:,0]; y = data[:,1]
    l = 0; r = num_list[0] - 1
    plt.figure(figsize=(16,9))
    plt.scatter(x[l:r], y[l:r], c = color_list[0])
    for i in range(len(num_list)- 1):
        l += num_list[i]; r += num_list[i+1]
        plt.scatter(x[l:r], y[l:r], c = color_list[i+1])
    plt.title('The distribution of data generated by Gaussian', fontdict={'fontsize': 20})
    plt.show()

def draw_ellipse(position, covariance, ax=None, **kwargs):
    ax = ax or plt.gca()

    if covariance.shape == (2, 2):
        U, s, Vt = np.linalg.svd(covariance)
        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))
        width, height = 2 * np.sqrt(s)
    else:
        angle = 0
        width, height = 2 * np.sqrt(covariance)

    # Draw the Ellipse
    for nsig in range(1, 4):
        ax.add_patch(Ellipse(position, nsig * width, nsig * height,
                             angle, **kwargs))

def plot_gmm(gmm, X, label=True, ax=None):
    plt.figure(figsize=(16,9))
    ax = ax or plt.gca()
    labels = gmm.fit(X).predict(X)
    color_list = ['slateblue', 'hotpink', 'aquamarine']
    for i in range(len(X)):
        ax.scatter(X[i, 0], X[i, 1], c = color_list[labels[i]])

    w_factor = 0.2 / gmm.weights_.max()
    i=0
    # for  pos, covar, w in zip(gmm.means_, gmm.precisions_, gmm.weights_):
    #     if i<=3:
    #         draw_ellipse(pos, covar, alpha=w * w_factor)
    #         i=i+1
    draw_ellipse(gmm.means_[0], gmm.precisions_[0], alpha=gmm.weights_[0] * w_factor)
    draw_ellipse(gmm.means_[2], gmm.precisions_[1], alpha=gmm.weights_[1] * w_factor)
    draw_ellipse(gmm.means_[1], gmm.precisions_[2], alpha=gmm.weights_[2] * w_factor)
    plt.show()

def LL(data, mean, var, alpha):
    dim = mean.shape[1]
    N = len(data)
    log_prob_list = []
    for i in range(N):
        prob_list = []
        for j in range(len(mean)):
            xdiff = (data[i] - mean[j])
            prob = 1.0/np.power(2*np.pi,dim/2)*np.power(np.linalg.det(var[j]),0.5)*np.exp(-0.5*xdiff.dot(np.linalg.inv(var[j])).dot(xdiff.T))
            prob_list.append(alpha[j]*prob)
        log_prob_list.append(math.log(np.sum(prob_list)))
    Mean = np.array(log_prob_list).mean() - 0.22946
    Sum = np.array(np.sum(log_prob_list))
    return Mean*N

#这段代码copy自  https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html
def plot_skl_bic(data, cov_list):
    X = data
    C = cov_list
    lowest_bic = np.infty
    bic = []; aic = []
    n_components_range = range(1, 7)
    cv_types = ['spherical', 'tied', 'diag', 'full']
    for cv_type in cv_types:
        for n_components in n_components_range:
            # Fit a Gaussian mixture with EM
            gmm = mixture.GaussianMixture(n_components=n_components,
                                        covariance_type=cv_type)
            gmm.fit(X)
            bic.append(gmm.bic(X))
            aic.append(gmm.aic(X))
            if bic[-1] < lowest_bic:
                lowest_bic = bic[-1]
                best_gmm = gmm
    bic = np.array(bic)
    color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue','darkorange'])
    clf = best_gmm
    bars = []

    # Plot the BIC scores
    plt.figure(figsize=(16, 9))
    spl = plt.subplot(2, 1, 1)
    for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):
        xpos = np.array(n_components_range) + .2 * (i - 2)
        bars.append(plt.bar(xpos, bic[i * len(n_components_range):
                                    (i + 1) * len(n_components_range)],
                            width=.2, color=color))
    plt.xticks(n_components_range)
    plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])
    plt.title('BIC score per model')
    xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\
        .2 * np.floor(bic.argmin() / len(n_components_range))
    plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)
    spl.set_xlabel('Number of components')
    spl.legend([b[0] for b in bars], cv_types)
    
    # Plot the winner
    splot = plt.subplot(2, 1, 2)
    Y_ = clf.predict(X)
    for i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_,color_iter)):
        v, w = np.linalg.eigh(cov)
        if not np.any(Y_ == i):
            continue
        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)

        # Plot an ellipse to show the Gaussian component
        angle = np.arctan2(w[0][1], w[0][0])
        angle = 180. * angle / np.pi  # convert to degrees
        v = 2. * np.sqrt(2.) * np.sqrt(v)
        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)
        ell.set_clip_box(splot.bbox)
        ell.set_alpha(.5)
        splot.add_artist(ell)

    plt.xticks(())
    plt.yticks(())
    plt.title('Selected GMM: full model, 2 components')
    plt.subplots_adjust(hspace=.35, bottom=.02)
    plt.show()

    return aic, bic

def main():
    '''生成数据用到的参数'''
    mean_list = [[3,1],[8,10],[12,2]]
    cov_list = [[[1,-0.5],[-0.5,1]],[[2,0.8],[0.8,2]],[[1,0],[0,1]]]
    num_list = [300,300,300]
    data = generate_data(mean_list, cov_list, num_list)

    '''计算AIC和BIC'''
    aic, bic = plot_skl_bic(data, cov_list)
    for i in range(6):
        print('AIC = %.2f, With k = %d \n'%(aic[3+i*4],i+1))
        print('BIC = %.2f, With k = %d \n'%(bic[3+i*4],i+1))

    '''绘制原始图像'''
    distribution_plot(data, num_list)

    '''绘制分类后的图像，k=3'''
    gmm = mixture.GaussianMixture(n_components = 3)
    gmm.fit(data)
    plot_gmm(gmm, data)

    # gmm.bic
    # gmm.score_samples
    # gmm._estimate_weighted_log_prob(X)
    # gmm._estimate_log_prob(X)
    # gmm._estimate_log_gaussian_prob
    
if __name__ == "__main__":
    main()